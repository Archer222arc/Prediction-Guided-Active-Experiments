#!/usr/bin/env python3
"""
ç»Ÿè®¡ä¼°è®¡å™¨å¯¹æ¯”å·¥å…· - å‚ç…§notebookå®ç°
Compare different statistical estimators - matching notebook implementation
"""

import numpy as np
import pandas as pd
import time
import json
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from typing import Dict, List, Optional
import logging
from scipy.stats import norm
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import gc
import psutil

# å¯¼å…¥ä¼°è®¡å™¨å’Œå·¥å…·å‡½æ•°
from pgae_estimator import PGAEEstimator
from adaptive_pgae_estimator import AdaptivePGAEEstimator
from active_inference_estimator import ActiveInferenceEstimator
from naive_estimator import NaiveEstimator
from utils import (
    get_PGAE_design, rejection_sample, PGAE_est_ci, overwrite_merge,
    summary_results, validate_data
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('default')
sns.set_palette("husl")

def run_notebook_style_comparison(df: pd.DataFrame, 
                                 target: str = 'ECON1MOD',
                                 X: List[str] = None,
                                 F: str = None, 
                                 Y: str = None,
                                 gamma: float = 0.5,
                                 n_experiments: int = 1000,
                                 n_true_labels: int = 500,
                                 alpha: float = 0.90,
                                 seed: Optional[int] = 42) -> Dict:
    """
    è¿è¡Œnotebooké£æ ¼çš„ä¼°è®¡å™¨å¯¹æ¯” - æ”¯æŒä¸åŒé¢„æµ‹ç›®æ ‡
    
    Args:
        df: è¾“å…¥æ•°æ®æ¡†
        target: é¢„æµ‹ç›®æ ‡ä»»åŠ¡åç§° ('ECON1MOD', 'UNITY', 'GPT1', 'MOREGUNIMPACT', 'GAMBLERESTR')
        X: åå˜é‡åˆ—ååˆ—è¡¨ (å¦‚æœä¸ºNoneï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®)
        F: é¢„æµ‹åˆ—å (å¦‚æœä¸ºNoneï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®) 
        Y: çœŸå®æ ‡ç­¾åˆ—å (å¦‚æœä¸ºNoneï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®)
        gamma: PGAEå‚æ•°
        n_experiments: å®éªŒæ¬¡æ•°
        n_true_labels: æ¯æ¬¡å®éªŒçš„çœŸå®æ ‡ç­¾æ•°é‡
        alpha: ç½®ä¿¡æ°´å¹³
        seed: éšæœºç§å­
        
    Returns:
        åŒ…å«æ‰€æœ‰ä¼°è®¡å™¨ç»“æœçš„å­—å…¸
    """
    
    # é¢„æµ‹ç›®æ ‡é…ç½® - åŸºäºnotebookå’Œé¢„æµ‹è´¨é‡åˆ†æ
    TARGET_CONFIGS = {
        'ECON1MOD': {
            'X': ['EDUCATION'],
            'F': 'ECON1MOD_LLM',
            'Y': 'ECON1MOD',
            'description': 'Economic conditions rating (1-4)',
            'optimal_method': 'finetuned'
        },
        'UNITY': {
            'X': ['EDUCATION'],  
            'F': 'UNITY_LLM',
            'Y': 'UNITY',
            'description': 'American unity perception (1-2)',
            'optimal_method': 'baseline'
        },
        'GPT1': {
            'X': ['EDUCATION'],
            'F': 'GPT1_LLM', 
            'Y': 'GPT1',
            'description': 'ChatGPT awareness (1-3)',
            'optimal_method': 'optimized'
        },
        'MOREGUNIMPACT': {
            'X': ['EDUCATION'],
            'F': 'MOREGUNIMPACT_LLM',
            'Y': 'MOREGUNIMPACT', 
            'description': 'More guns crime impact (1-3)',
            'optimal_method': 'finetuned'
        },
        'GAMBLERESTR': {
            'X': ['EDUCATION'],
            'F': 'GAMBLERESTR_LLM',
            'Y': 'GAMBLERESTR',
            'description': 'Gambling restrictions view (1-3)', 
            'optimal_method': 'finetuned'
        }
    }
    
    # ä½¿ç”¨ç›®æ ‡é…ç½®æˆ–ç”¨æˆ·è¦†ç›–
    if target not in TARGET_CONFIGS:
        raise ValueError(f"Unsupported target: {target}. Available targets: {list(TARGET_CONFIGS.keys())}")
    
    config = TARGET_CONFIGS[target]
    X = X if X is not None else config['X']
    F = F if F is not None else config['F'] 
    Y = Y if Y is not None else config['Y']
    
    logger.info(f"è¿è¡Œ {target} é¢„æµ‹ç›®æ ‡å¯¹æ¯”")
    logger.info(f"æè¿°: {config['description']}")
    logger.info(f"æ¨èé¢„æµ‹æ–¹æ³•: {config['optimal_method']}")
    logger.info(f"X={X}, F={F}, Y={Y}")
    logger.info("å¼€å§‹notebooké£æ ¼çš„ç»Ÿè®¡ä¼°è®¡å™¨å¯¹æ¯”")
    
    # æ•°æ®é¢„å¤„ç† - å®Œå…¨æŒ‰ç…§notebook
    df_clean = df[X + [F] + [Y]].copy()
    df_clean = df_clean[df_clean[X + [F] + [Y]].lt(10).all(axis=1)]
    logger.info(f"æ•°æ®æ¸…ç†åæ ·æœ¬æ•°: {len(df_clean)}")
    
    # è®¡ç®—true_pmf - æŒ‰ç…§notebook
    group_stats = df_clean.groupby(X).agg(
        cnt=(F, 'count'),
    ).reset_index()
    group_stats['true_pmf'] = group_stats['cnt'] / group_stats['cnt'].sum()
    df_clean = df_clean.merge(group_stats, on=X, how='left')
    
    # è®¡ç®—çœŸå®å€¼
    true_value = df_clean[Y].mean()
    logger.info(f"çœŸå®ç›®æ ‡å€¼: {true_value:.6f}")
    
    if seed is not None:
        np.random.seed(seed)
    
    # åˆå§‹åŒ–ç»“æœæ•°ç»„
    results = {}
    
    # 1. PGAEæ–¹æ³• - ä½¿ç”¨å¹¶å‘ä¼˜åŒ–çš„PGAEEstimator
    logger.info("è¿è¡ŒPGAEä¼°è®¡å™¨...")
    pgae_estimator = PGAEEstimator(X=X, F=F, Y=Y, gamma=gamma)
    
    # ä½¿ç”¨å¹¶å‘æ‰§è¡ŒPGAEå®éªŒ
    pgae_results = pgae_estimator.run_experiments(
        df_clean, 
        n_experiments=n_experiments,
        n_true_labels=n_true_labels,
        alpha=alpha,
        seed=seed,
        use_concurrent=True,
        max_workers=10  # é™åˆ¶workeræ•°é‡ä»¥èŠ‚çœå†…å­˜
    )
    
    results['PGAE'] = pgae_results
    
    # 2. Adaptive PGAEæ–¹æ³• - ä½¿ç”¨æ”¹è¿›çš„è‡ªé€‚åº”è®¾è®¡æ›´æ–°ç­–ç•¥ï¼ˆè°ƒå‚ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    logger.info("è¿è¡Œè‡ªé€‚åº”PGAEä¼°è®¡å™¨...")
    adaptive_pgae_estimator = AdaptivePGAEEstimator(
        X=X, F=F, Y=Y, gamma=gamma, batch_size=250,
        design_update_freq=1,  # æ¯æ‰¹æ›´æ–°ä¸€æ¬¡è®¾è®¡ï¼ˆæœ€ä¼˜å‚æ•°ï¼‰
        warmup_batches=2       # å‰2æ‰¹ä½¿ç”¨å›ºå®šè®¾è®¡ï¼ˆæœ€ä¼˜å‚æ•°ï¼‰
    )
    
    # ä½¿ç”¨å¹¶å‘æ‰§è¡Œè‡ªé€‚åº”PGAEå®éªŒ
    adaptive_results = adaptive_pgae_estimator.run_experiments(
        df_clean,
        n_experiments=n_experiments,
        n_true_labels=n_true_labels,
        alpha=alpha,
        seed=seed,
        use_concurrent=True,
        max_workers=10  # é™åˆ¶workeræ•°é‡ä»¥èŠ‚çœå†…å­˜
    )
    
    results['Adaptive_PGAE'] = adaptive_results
    
    # 3. Active Statistical Inferenceæ–¹æ³• - ä½¿ç”¨å¹¶å‘ä¼˜åŒ–çš„ActiveInferenceEstimator  
    logger.info("è¿è¡Œä¸»åŠ¨ç»Ÿè®¡æ¨æ–­ä¼°è®¡å™¨...")
    active_inference_estimator = ActiveInferenceEstimator(X=X, F=F, Y=Y, gamma=gamma)
    
    # ä½¿ç”¨å¹¶å‘æ‰§è¡Œä¸»åŠ¨ç»Ÿè®¡æ¨æ–­å®éªŒ
    active_results = active_inference_estimator.run_experiments(
        df_clean,
        n_experiments=n_experiments,
        n_true_labels=n_true_labels,
        alpha=alpha,
        seed=seed,
        use_concurrent=True,
        max_workers=10  # é™åˆ¶workeræ•°é‡ä»¥èŠ‚çœå†…å­˜
    )
    
    results['Active_Inference'] = active_results
    
    # 4. Naiveæ–¹æ³• - åŸºçº¿å¯¹æ¯”ï¼Œä½¿ç”¨æœ€ç®€å•çš„ç»Ÿè®¡æ–¹æ³•
    logger.info("è¿è¡ŒNaiveä¼°è®¡å™¨...")
    naive_estimator = NaiveEstimator(X=X, F=F, Y=Y, gamma=gamma)
    
    # ä½¿ç”¨å¹¶å‘æ‰§è¡ŒNaiveå®éªŒ
    naive_results = naive_estimator.run_experiments(
        df_clean,
        n_experiments=n_experiments,
        n_true_labels=n_true_labels,
        alpha=alpha,
        seed=seed,
        use_concurrent=True,
        max_workers=10  # é™åˆ¶workeræ•°é‡ä»¥èŠ‚çœå†…å­˜
    )
    
    results['Naive'] = naive_results
    
    # æ±‡æ€»ç»“æœ
    total_time = (pgae_results.get('execution_time', 0) + 
                 adaptive_results.get('execution_time', 0) + 
                 active_results.get('execution_time', 0))
    logger.info(f"æ‰€æœ‰å®éªŒå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    
    results['summary'] = {
        'true_value': true_value,
        'n_experiments': n_experiments,
        'n_true_labels': n_true_labels,
        'alpha': alpha,
        'gamma': gamma,
        'total_execution_time': total_time,
        'parameters': {
            'X': X, 'F': F, 'Y': Y,
            'batch_size': 100,
            'seed': seed
        }
    }
    
    return results

def create_comparison_plots(results: Dict, output_dir: str = "./"):
    """
    åˆ›å»ºå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨ - å‚ç…§notebookç»“æœ
    
    Args:
        results: å¯¹æ¯”ç»“æœå­—å…¸
        output_dir: è¾“å‡ºç›®å½•
    """
    logger.info("ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨...")
    
    # æå–æ–¹æ³•å’ŒæŒ‡æ ‡
    methods = ['PGAE', 'Adaptive_PGAE', 'Active_Inference', 'Naive']
    method_labels = ['PGAE', 'Adaptive PGAE', 'Active Inference', 'Naive']
    
    metrics = {
        'MSE': [results[method]['mse'] for method in methods],
        'Coverage Rate': [results[method]['coverage_rate'] for method in methods],
        'Avg CI Length': [results[method]['avg_ci_length'] for method in methods],
        'Execution Time (s)': [results[method]['execution_time'] for method in methods]
    }
    
    # åˆ›å»º2x2å­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']
    
    # MSEæ¯”è¾ƒ
    ax1 = axes[0, 0]
    bars1 = ax1.bar(method_labels, metrics['MSE'], color=colors, alpha=0.7, edgecolor='black')
    ax1.set_title('Mean Squared Error (MSE)', fontsize=14, fontweight='bold')
    ax1.set_ylabel('MSE', fontsize=12)
    ax1.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, bar in enumerate(bars1):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                f'{height:.6f}', ha='center', va='bottom', fontsize=10)
    
    # è¦†ç›–ç‡æ¯”è¾ƒ
    ax2 = axes[0, 1]
    bars2 = ax2.bar(method_labels, metrics['Coverage Rate'], color=colors, alpha=0.7, edgecolor='black')
    ax2.set_title('Coverage Rate', fontsize=14, fontweight='bold')
    ax2.set_ylabel('Coverage Rate', fontsize=12)
    ax2.tick_params(axis='x', rotation=45)
    ax2.axhline(y=0.9, color='red', linestyle='--', alpha=0.7, label='Target (90%)')
    ax2.legend()
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, bar in enumerate(bars2):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=10)
    
    # ç½®ä¿¡åŒºé—´é•¿åº¦æ¯”è¾ƒ
    ax3 = axes[1, 0]
    bars3 = ax3.bar(method_labels, metrics['Avg CI Length'], color=colors, alpha=0.7, edgecolor='black')
    ax3.set_title('Average CI Length', fontsize=14, fontweight='bold')
    ax3.set_ylabel('CI Length', fontsize=12)
    ax3.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, bar in enumerate(bars3):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                f'{height:.4f}', ha='center', va='bottom', fontsize=10)
    
    # æ‰§è¡Œæ—¶é—´æ¯”è¾ƒ
    ax4 = axes[1, 1]
    bars4 = ax4.bar(method_labels, metrics['Execution Time (s)'], color=colors, alpha=0.7, edgecolor='black')
    ax4.set_title('Execution Time', fontsize=14, fontweight='bold')
    ax4.set_ylabel('Time (seconds)', fontsize=12)
    ax4.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, bar in enumerate(bars4):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                f'{height:.1f}', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/estimator_comparison.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # åˆ›å»ºæ€§èƒ½æ€»ç»“è¡¨
    summary_df = pd.DataFrame({
        'Method': method_labels,
        'MSE': [f"{mse:.6f}" for mse in metrics['MSE']],
        'Coverage Rate': [f"{cr:.4f}" for cr in metrics['Coverage Rate']],
        'Avg CI Length': [f"{cl:.4f}" for cl in metrics['Avg CI Length']],
        'Time (s)': [f"{t:.1f}" for t in metrics['Execution Time (s)']]
    })
    
    print("\n" + "="*80)
    print("STATISTICAL ESTIMATOR COMPARISON RESULTS")
    print("="*80)
    print(summary_df.to_string(index=False))
    print("="*80)
    
    logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {output_dir}/estimator_comparison.png")

class EstimatorComparison:
    """ä¼°è®¡å™¨å¯¹æ¯”åˆ†æç±» - ä¿æŒå‘åå…¼å®¹"""
    
    def __init__(self, data_file: str, X: List[str] = ['EDUCATION'], 
                 F: str = 'ECON1MOD_LLM', Y: str = 'ECON1MOD', gamma: float = 0.5):
        """
        åˆå§‹åŒ–å¯¹æ¯”åˆ†æ
        
        Args:
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            X: åå˜é‡åˆ—ååˆ—è¡¨
            F: é¢„æµ‹åˆ—å
            Y: çœŸå®æ ‡ç­¾åˆ—å
            gamma: å‚æ•°gamma
        """
        self.data_file = data_file
        self.X = X
        self.F = F
        self.Y = Y
        self.gamma = gamma
        
        # åŠ è½½æ•°æ®
        logger.info(f"åŠ è½½æ•°æ®: {data_file}")
        self.df = pd.read_csv(data_file)
        
        # åˆå§‹åŒ–ä¼°è®¡å™¨
        self.estimators = {
            'PGAE': PGAEEstimator(X=self.X, F=self.F, Y=self.Y, gamma=self.gamma),
            'Adaptive_PGAE': AdaptivePGAEEstimator(X=self.X, F=self.F, Y=self.Y, gamma=self.gamma),
            'Active_Inference': ActiveInferenceEstimator(X=self.X, F=self.F, Y=self.Y, gamma=self.gamma),
            'Naive': NaiveEstimator(X=self.X, F=self.F, Y=self.Y, gamma=self.gamma)
        }
        
        logger.info("ä¼°è®¡å™¨å¯¹æ¯”åˆ†æåˆå§‹åŒ–å®Œæˆ")
    
    def run_comparison(self, n_experiments: int = 100, n_true_labels: int = 500, 
                      alpha: float = 0.9, seed: int = 42) -> Dict:
        """
        è¿è¡Œæ‰€æœ‰ä¼°è®¡å™¨çš„å¯¹æ¯”å®éªŒ
        
        Args:
            n_experiments: å®éªŒæ¬¡æ•°
            n_true_labels: æ¯æ¬¡å®éªŒçš„çœŸå®æ ‡ç­¾æ•°é‡
            alpha: ç½®ä¿¡æ°´å¹³
            seed: éšæœºç§å­
            
        Returns:
            æ‰€æœ‰æ–¹æ³•çš„å®éªŒç»“æœ
        """
        logger.info(f"å¼€å§‹å¯¹æ¯”å®éªŒ: {n_experiments}æ¬¡å®éªŒ, æ¯æ¬¡{n_true_labels}ä¸ªæ ‡ç­¾")
        
        results = {}
        
        for method_name, estimator in self.estimators.items():
            logger.info(f"\nè¿è¡Œ {method_name} ä¼°è®¡å™¨...")
            start_time = time.time()
            
            try:
                method_results = estimator.run_experiments(
                    self.df, n_experiments, n_true_labels, alpha, seed
                )
                results[method_name] = method_results
                
                end_time = time.time()
                logger.info(f"{method_name} å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
                
            except Exception as e:
                logger.error(f"{method_name} è¿è¡Œå¤±è´¥: {e}")
                continue
        
        # æ·»åŠ å¯¹æ¯”æ€»ç»“
        results['comparison_summary'] = self._generate_comparison_summary(results)
        
        return results
    
    def _generate_comparison_summary(self, results: Dict) -> Dict:
        """ç”Ÿæˆå¯¹æ¯”æ€»ç»“"""
        summary = {
            'methods_compared': list(results.keys()),
            'best_mse': {},
            'best_coverage': {},
            'best_ci_length': {},
            'execution_times': {}
        }
        
        # æ‰¾å‡ºå„é¡¹æŒ‡æ ‡æœ€ä½³çš„æ–¹æ³•
        mse_values = {method: res['mse'] for method, res in results.items() if 'mse' in res}
        if mse_values:
            best_mse_method = min(mse_values.keys(), key=lambda x: mse_values[x])
            summary['best_mse'] = {'method': best_mse_method, 'value': mse_values[best_mse_method]}
        
        coverage_values = {method: res['coverage_rate'] for method, res in results.items() if 'coverage_rate' in res}
        if coverage_values:
            best_coverage_method = max(coverage_values.keys(), key=lambda x: coverage_values[x])
            summary['best_coverage'] = {'method': best_coverage_method, 'value': coverage_values[best_coverage_method]}
        
        ci_length_values = {method: res['avg_ci_length'] for method, res in results.items() if 'avg_ci_length' in res}
        if ci_length_values:
            best_ci_method = min(ci_length_values.keys(), key=lambda x: ci_length_values[x])
            summary['best_ci_length'] = {'method': best_ci_method, 'value': ci_length_values[best_ci_method]}
        
        # æ‰§è¡Œæ—¶é—´
        summary['execution_times'] = {method: res.get('execution_time', 0) for method, res in results.items() if 'execution_time' in res}
        
        return summary
    
    def create_visualization(self, results: Dict, save_path: str = "estimator_comparison.png"):
        """
        åˆ›å»ºå¯è§†åŒ–å¯¹æ¯”å›¾è¡¨
        
        Args:
            results: å®éªŒç»“æœ
            save_path: ä¿å­˜è·¯å¾„
        """
        # è¿‡æ»¤å‡ºæœ‰æ•ˆç»“æœ
        valid_results = {k: v for k, v in results.items() 
                        if k != 'comparison_summary' and 'mse' in v}
        
        if not valid_results:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„ç»“æœç”¨äºå¯è§†åŒ–")
            return
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Statistical Estimators Comparison', fontsize=16, fontweight='bold')
        
        methods = list(valid_results.keys())
        
        # 1. MSEå¯¹æ¯”
        mse_values = [valid_results[method]['mse'] for method in methods]
        axes[0, 0].bar(methods, mse_values, color=sns.color_palette("husl", len(methods)))
        axes[0, 0].set_title('Mean Squared Error (MSE)')
        axes[0, 0].set_ylabel('MSE')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. Coverage Rateå¯¹æ¯”
        coverage_values = [valid_results[method]['coverage_rate'] for method in methods]
        axes[0, 1].bar(methods, coverage_values, color=sns.color_palette("husl", len(methods)))
        axes[0, 1].set_title('Coverage Rate')
        axes[0, 1].set_ylabel('Coverage Rate')
        axes[0, 1].set_ylim([0, 1])
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. CI Lengthå¯¹æ¯”
        ci_length_values = [valid_results[method]['avg_ci_length'] for method in methods]
        axes[1, 0].bar(methods, ci_length_values, color=sns.color_palette("husl", len(methods)))
        axes[1, 0].set_title('Average Confidence Interval Length')
        axes[1, 0].set_ylabel('CI Length')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. æ‰§è¡Œæ—¶é—´å¯¹æ¯”
        execution_times = [valid_results[method].get('execution_time', 0) for method in methods]
        axes[1, 1].bar(methods, execution_times, color=sns.color_palette("husl", len(methods)))
        axes[1, 1].set_title('Execution Time')
        axes[1, 1].set_ylabel('Time (seconds)')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def create_detailed_comparison_plot(self, results: Dict, save_path: str = "detailed_comparison.png"):
        """
        åˆ›å»ºè¯¦ç»†çš„ä¼°è®¡å€¼åˆ†å¸ƒå¯¹æ¯”å›¾
        
        Args:
            results: å®éªŒç»“æœ
            save_path: ä¿å­˜è·¯å¾„
        """
        valid_results = {k: v for k, v in results.items() 
                        if k != 'comparison_summary' and 'tau_estimates' in v}
        
        if not valid_results:
            logger.warning("æ²¡æœ‰è¯¦ç»†ç»“æœç”¨äºå¯è§†åŒ–")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ä¼°è®¡å€¼åˆ†å¸ƒçš„ç®±çº¿å›¾
        tau_data = []
        method_labels = []
        
        for method, result in valid_results.items():
            tau_data.append(result['tau_estimates'])
            method_labels.append(method)
        
        ax1.boxplot(tau_data, labels=method_labels)
        ax1.set_title('Distribution of Estimates')
        ax1.set_ylabel('Estimate Value')
        ax1.tick_params(axis='x', rotation=45)
        
        # æ·»åŠ çœŸå®å€¼çº¿
        if valid_results:
            true_value = list(valid_results.values())[0]['true_value']
            ax1.axhline(y=true_value, color='red', linestyle='--', 
                       label=f'True Value: {true_value:.4f}')
            ax1.legend()
        
        # CIå®½åº¦åˆ†å¸ƒ
        ci_widths = []
        for method, result in valid_results.items():
            ci_width = result['h_ci'] - result['l_ci']
            ci_widths.append(ci_width)
        
        ax2.boxplot(ci_widths, labels=method_labels)
        ax2.set_title('Distribution of CI Widths')
        ax2.set_ylabel('CI Width')
        ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"è¯¦ç»†å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def save_comparison_results(self, results: Dict, filename: str = "") -> str:
        """
        ä¿å­˜å¯¹æ¯”ç»“æœ
        
        Args:
            results: å¯¹æ¯”ç»“æœ
            filename: æ–‡ä»¶å
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶å
        """
        if not filename:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f'estimators_comparison_{timestamp}.json'
        
        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {}
        for method, result in results.items():
            if method == 'comparison_summary':
                save_data[method] = result
            else:
                save_data[method] = {
                    'method': result.get('method', method),
                    'mse': result.get('mse', 0),
                    'bias': result.get('bias', 0),
                    'variance': result.get('variance', 0),
                    'avg_ci_length': result.get('avg_ci_length', 0),
                    'coverage_rate': result.get('coverage_rate', 0),
                    'true_value': result.get('true_value', 0),
                    'execution_time': result.get('execution_time', 0),
                    'parameters': result.get('parameters', {}),
                    'n_experiments': result.get('n_experiments', 0)
                }
        
        with open(filename, 'w') as f:
            json.dump(save_data, f, indent=2)
        
        logger.info(f"å¯¹æ¯”ç»“æœå·²ä¿å­˜: {filename}")
        return filename

def main():
    """ä¸»å‡½æ•°"""
    import sys
    import argparse
    import pandas as pd
    
    parser = argparse.ArgumentParser(description='ç»Ÿè®¡ä¼°è®¡å™¨å¯¹æ¯”å·¥å…·')
    parser.add_argument('data_file', help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--dataset-choice', choices=['base', 'cot'], default=None,
                       help='å¿«æ·é€‰æ‹©å†…ç½®æ•°æ®é›†: base æˆ– cot (è‹¥æä¾›ï¼Œå°†è¦†ç›– data_file)')
    parser.add_argument('--target', '-t', default='ECON1MOD', 
                       choices=['ECON1MOD', 'UNITY', 'GPT1', 'MOREGUNIMPACT', 'GAMBLERESTR'],
                       help='é¢„æµ‹ç›®æ ‡ä»»åŠ¡ (é»˜è®¤: ECON1MOD)')
    parser.add_argument('--experiments', '-e', type=int, default=100,
                       help='å®éªŒæ¬¡æ•° (é»˜è®¤: 100)')
    parser.add_argument('--labels', '-l', type=int, default=500,
                       help='æ¯æ¬¡å®éªŒçš„çœŸå®æ ‡ç­¾æ•°é‡ (é»˜è®¤: 500)')
    parser.add_argument('--gamma', '-g', type=float, default=0.5,
                       help='PGAE gammaå‚æ•° (é»˜è®¤: 0.5)')
    parser.add_argument('--alpha', '-a', type=float, default=0.90,
                       help='ç½®ä¿¡æ°´å¹³ (é»˜è®¤: 0.90)')
    parser.add_argument('--seed', '-s', type=int, default=42,
                       help='éšæœºç§å­ (é»˜è®¤: 42)')
    parser.add_argument('--output', '-o', type=str, default=None,
                       help='è¾“å‡ºæ–‡ä»¶åå‰ç¼€')
    # å›ºå®šCIå®½åº¦ â†’ æœ€å°æ ‡ç­¾æ•° æ¨¡å¼å‚æ•°ï¼ˆå¯é€‰ï¼‰
    parser.add_argument('--ci-width', type=float, default=None, help='ç›®æ ‡CIå®½åº¦ï¼ˆä¾‹å¦‚ 0.05ï¼‰ã€‚æä¾›è¯¥å‚æ•°åˆ™è¿è¡Œâ€œæœ€å°æ ‡ç­¾æˆæœ¬â€æ¨¡å¼')
    parser.add_argument('--ci-tolerance', type=float, default=0.005, help='CIå®½åº¦å®¹å¿åº¦ï¼Œä¾‹å¦‚0.005è¡¨ç¤ºå…è®¸ avg_CI <= ç›®æ ‡+0.005 è§†ä¸ºè¾¾æ ‡')
    parser.add_argument('--methods', nargs='*', default=['PGAE','Adaptive_PGAE'], help='å‚ä¸æ–¹æ³•ï¼Œé»˜è®¤ [PGAE, Adaptive_PGAE]')
    parser.add_argument('--gamma-grid', nargs='*', type=float, default=[0.5], help='æ‰«æçš„gammaå–å€¼ï¼Œé»˜è®¤[0.5]')
    # CIæ¨¡å¼ä¸‹é»˜è®¤æ²¿ç”¨MSEæ¨¡å¼è°ƒä¼˜è¿‡çš„Adaptiveå‚æ•°ï¼šbatch_size=250, design_update_freq=1, warmup_batches=2
    parser.add_argument('--batch-size', type=int, default=250, help='Adaptive: æ‰¹å¤§å°ï¼ˆé»˜è®¤: 250ï¼Œæ¥è‡ªMSEè°ƒä¼˜ï¼‰')
    parser.add_argument('--design-update-freq', type=int, default=1, help='Adaptive: è®¾è®¡æ›´æ–°é¢‘ç‡ï¼ˆé»˜è®¤: 1ï¼Œæ¥è‡ªMSEè°ƒä¼˜ï¼‰')
    parser.add_argument('--warmup-batches', type=int, default=2, help='Adaptive: é¢„çƒ­æ‰¹æ¬¡æ•°ï¼ˆé»˜è®¤: 2ï¼Œæ¥è‡ªMSEè°ƒä¼˜ï¼‰')
    parser.add_argument('--min-labels', type=int, default=100, help='æœç´¢çš„æœ€å°æ ‡ç­¾æ•°')
    parser.add_argument('--max-labels', type=int, default=2000, help='æœç´¢çš„æœ€å¤§æ ‡ç­¾æ•°')
    parser.add_argument('--label-step', type=int, default=100, help='n_labels æœç´¢æ­¥é•¿ï¼ˆé»˜è®¤100ï¼‰')
    parser.add_argument('--concurrent', action='store_true', help='åœ¨CIæˆæœ¬æ¨¡å¼ä¸‹å¯ç”¨å¹¶å‘æ‰§è¡Œï¼ˆé»˜è®¤å¯ç”¨ï¼‰')
    parser.add_argument('--max-workers', type=int, default=10, help='å¹¶å‘workeræ•°é‡ä¸Šé™ï¼ˆé»˜è®¤10ï¼‰')
    parser.add_argument('--results-csv', type=str, default='compare_runs_log.csv', help='æ¯”è¾ƒç»“æœæ±‡æ€»CSVï¼ˆåŒè®¾ç½®å°†è¦†ç›–ï¼‰')
    
    # æ€»æ˜¯ä½¿ç”¨argparseè§£æå‚æ•°
    args = parser.parse_args()
    # å¤„ç†æ•°æ®é›†å¿«æ·é€‰æ‹©
    data_file = args.data_file
    if args.dataset_choice is not None:
        preset_map = {
            'base': 'archive/predictions/NPORS_2024_base_gpt41mini_lr06_step560_20250911_214943.csv',
            'cot': 'archive/predictions/NPORS_2024_cot_optimized_lr06_step560_20250911_232934.csv',
        }
        chosen = preset_map.get(args.dataset_choice)
        if chosen is not None:
            data_file = chosen
            logger.info(f"ä½¿ç”¨å†…ç½®æ•°æ®é›†é€‰æ‹©: {args.dataset_choice} -> {data_file}")
        else:
            logger.warning(f"æœªçŸ¥çš„æ•°æ®é›†é€‰é¡¹: {args.dataset_choice}ï¼Œç»§ç»­ä½¿ç”¨æä¾›çš„ data_file")
    target = args.target
    n_experiments = args.experiments
    n_true_labels = args.labels
    gamma = args.gamma
    alpha = args.alpha
    seed = args.seed
    output = args.output
    
    logger.info(f"åŠ è½½æ•°æ®: {data_file}")
    df = pd.read_csv(data_file)
    
    # å¦‚æœæä¾›äº† --ci-widthï¼Œåˆ™è¿è¡Œâ€œå›ºå®šCIâ†’æœ€å°æ ‡ç­¾æˆæœ¬â€æ¨¡å¼
    if args.ci_width is not None:
        # éƒ¨ç½²åŸºäºCIæ¨¡å¼çš„æœ€ä½³é»˜è®¤å‚æ•°
        # PGAE: æ¥è‡ªè°ƒå‚ç»“æœï¼ˆå›ºå®šgamma=0.5æ—¶çš„æœ€ä½³ï¼‰
        DEFAULT_PGAE_EST_CI_PARAMS = {
            'n_estimators_mu': 100,
            'n_estimators_tau': 200,
            'K': 5,
            'max_depth': 10,
        }
        DEFAULT_PGAE_DESIGN_PARAMS = {
            'min_var_threshold': 0.0001,
            'prob_clip_min': 0.1,
            'prob_clip_max': 0.9,
        }

        def run_once_and_get_ci_length(estimator, df_in: pd.DataFrame, n_exp: int, n_labels: int,
                                       a: float, sd: Optional[int], use_cc: bool, max_w: Optional[int]):
            kwargs = dict(n_experiments=n_exp, n_true_labels=n_labels, alpha=a, seed=sd, use_concurrent=use_cc)
            try:
                res = estimator.run_experiments(df_in, **kwargs, max_workers=max_w)
            except TypeError:
                raise ValueError("No workers argument supported in this estimator's run_experiments method.")
                res = estimator.run_experiments(df_in, **kwargs)
            return float(res['avg_ci_length']), res

        def search_min_labels_for_ci(factory, df_in: pd.DataFrame, target_ci: float,
                                      a: float, sd: Optional[int], n_exp: int,
                                      labels_lo: int, labels_hi: int, step: int,
                                      use_cc: bool, max_w: Optional[int], tol: float):
            audit = []
            # å¯¹é½åˆ°æ­¥é•¿ï¼ˆå‘ä¸Šå–æ•´ï¼‰
            def align_up(n: int, s: int) -> int:
                return int(((n + s - 1) // s) * s)

            def align_range(lo: int, hi: int, s: int) -> (int, int):
                alo = align_up(lo, s)
                ahi = (hi // s) * s if hi >= s else hi
                if ahi < alo:
                    ahi = alo
                return alo, ahi

            labels_lo, labels_hi = align_range(labels_lo, labels_hi, step)
            low = labels_lo
            high = labels_lo
            best = None
            # grow
            while True:
                est = factory()
                ci_len, res = run_once_and_get_ci_length(est, df_in, n_exp, high, a, sd, use_cc, max_w)
                audit.append({'labels': int(high), 'avg_ci_length': float(ci_len)})
                if ci_len <= target_ci + tol:
                    best = (high, ci_len, res)
                    break
                if high >= labels_hi:
                    break
                low = high
                high = min(align_up(high * 2, step), labels_hi)
            if best is None:
                return {'achieved': False, 'required_labels': None, 'avg_ci_length': audit[-1]['avg_ci_length'] if audit else None, 'audit': audit}
            # binary search
            lo = min(align_up(low + 1, step), labels_hi)
            hi = best[0]
            req_labels, req_ci, req_res = hi, best[1], best[2]
            while lo <= hi:
                # å–ä¸­ç‚¹å¹¶å¯¹é½åˆ°æ­¥é•¿ï¼ˆå‘ä¸Šå¯¹é½ä¿è¯ä¸ä½äºä¸­ç‚¹ï¼‰
                mid_raw = (lo + hi) // 2
                mid = align_up(mid_raw, step)
                if mid > hi:
                    mid = hi
                est = factory()
                ci_len, res = run_once_and_get_ci_length(est, df_in, n_exp, mid, a, sd, use_cc, max_w)
                audit.append({'labels': int(mid), 'avg_ci_length': float(ci_len)})
                if ci_len <= target_ci + tol:
                    req_labels, req_ci, req_res = mid, ci_len, res
                    # å‘ä¸‹ç¼©å°ä¸€æ ¼æ­¥é•¿
                    hi = mid - step
                else:
                    # å‘ä¸Šç§»åŠ¨ä¸€æ ¼æ­¥é•¿
                    lo = mid + step
            return {'achieved': True, 'required_labels': int(req_labels), 'avg_ci_length': float(req_ci), 'results_snapshot': {
                'mse': float(req_res['mse']), 'coverage_rate': float(req_res['coverage_rate']), 'variance': float(req_res['variance']), 'parameters': req_res.get('parameters', {})
            }, 'audit': sorted(audit, key=lambda x: x['labels']), 'tolerance': tol}

        # Build config
        TARGET_CONFIGS = {
            'ECON1MOD': {'X': ['EDUCATION'], 'F': 'ECON1MOD_LLM', 'Y': 'ECON1MOD', 'description': 'Economic conditions rating (1-4)'},
            'UNITY': {'X': ['EDUCATION'], 'F': 'UNITY_LLM', 'Y': 'UNITY', 'description': 'US unity perception (1-2)'},
            'GPT1': {'X': ['EDUCATION'], 'F': 'GPT1_LLM', 'Y': 'GPT1', 'description': 'ChatGPT familiarity (1-3)'},
            'MOREGUNIMPACT': {'X': ['EDUCATION'], 'F': 'MOREGUNIMPACT_LLM', 'Y': 'MOREGUNIMPACT', 'description': 'Gun control impact (1-3)'},
            'GAMBLERESTR': {'X': ['EDUCATION'], 'F': 'GAMBLERESTR_LLM', 'Y': 'GAMBLERESTR', 'description': 'Gambling restriction opinion (1-3)'}
        }
        cfg = TARGET_CONFIGS[target]
        X, F, Y = cfg['X'], cfg['F'], cfg['Y']

        logger.info(f"è¿è¡ŒCIæˆæœ¬æ¨¡å¼: ç›®æ ‡CIå®½åº¦={args.ci_width} (Â±{args.ci_tolerance}), alpha={alpha}")
        methods = args.methods
        gamma_grid = args.gamma_grid
        # é»˜è®¤å¯ç”¨å¹¶å‘ï¼›å³ä½¿æœªæä¾› --concurrent ä¹Ÿå¯ç”¨
        use_cc = True
        max_w = args.max_workers or 10
        results = {}
        for method in methods:
            logger.info(f"æ–¹æ³•: {method}")
            best = None
            for g in gamma_grid:
                if method == 'PGAE':
                    def factory(gg=g):
                        return PGAEEstimator(
                            X=X, F=F, Y=Y, gamma=gg,
                            design_params=DEFAULT_PGAE_DESIGN_PARAMS,
                            est_ci_params=DEFAULT_PGAE_EST_CI_PARAMS
                        )
                elif method == 'Adaptive_PGAE':
                    def factory(gg=g, bs=args.batch_size, uf=args.design_update_freq, wu=args.warmup_batches):
                        return AdaptivePGAEEstimator(X=X, F=F, Y=Y, gamma=gg, batch_size=bs,
                                                     design_update_freq=uf, warmup_batches=wu)
                elif method == 'Active_Inference':
                    def factory(gg=g):
                        return ActiveInferenceEstimator(X=X, F=F, Y=Y, gamma=gg)
                elif method == 'Naive':
                    def factory(gg=g):
                        return NaiveEstimator(X=X, F=F, Y=Y, gamma=gg)
                else:
                    logger.warning(f"  ä¸æ”¯æŒçš„æ–¹æ³•: {method}, è·³è¿‡")
                    continue
                out = search_min_labels_for_ci(lambda: factory(), df, args.ci_width, alpha, seed, n_experiments,
                                               args.min_labels, args.max_labels, args.label_step,
                                               use_cc, max_w, args.ci_tolerance)
                out['gamma'] = g
                if best is None or (out.get('achieved') and out.get('required_labels') is not None and out['required_labels'] < best['required_labels']):
                    best = out
            results[method] = best if best is not None else {'achieved': False}

        # æ‰“å°æ€»ç»“
        print("\n" + "=" * 90)
        print("CI WIDTH COST COMPARISON (lower labels = lower cost)")
        print("=" * 90)
        for method, res in results.items():
            if not res or not res.get('achieved'):
                print(f"{method:<18} -> æœªè¾¾åˆ°ç›®æ ‡CIå®½åº¦ (<= {args.ci_width}+{args.ci_tolerance})ï¼Œåœ¨ labels <= {args.max_labels} èŒƒå›´å†…")
            else:
                mse_text = None
                snap = res.get('results_snapshot', {})
                if 'mse' in snap:
                    mse_text = f"MSE={snap['mse']:.6f}"
                extra = f" | {mse_text}" if mse_text else ""
                tol_txt = f"Â±{res.get('tolerance', args.ci_tolerance)}"
                print(f"{method:<18} -> éœ€è¦æ ‡ç­¾: {res['required_labels']}, gamma={res.get('gamma')} | avg_CI={res['avg_ci_length']:.4f} ({tol_txt}){extra}")
        print("=" * 90)

        # ä¿å­˜ç»“æœ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        out_name = (args.output or 'ci_cost_comparison') + f"_{target.lower()}_{timestamp}.json"
        with open(out_name, 'w') as f:
            json.dump(results, f, indent=2)
        logger.info(f"CIæˆæœ¬å¯¹æ¯”ç»“æœå·²ä¿å­˜: {out_name}")

        # ä¿å­˜åˆ°CSVï¼ˆæŒ‰è®¾ç½®å»é‡è¦†ç›–ï¼‰
        try:
            import pandas as pd
            rows = []
            settings_key = (
                f"mode=ci-cost|target={target}|dataset={data_file}|ci={args.ci_width}|tol={args.ci_tolerance}|"
                f"alpha={alpha}|experiments={n_experiments}|gamma_grid={','.join(map(str, args.gamma_grid))}|"
                f"adaptive_defaults={args.batch_size}-{args.design_update_freq}-{args.warmup_batches}"
            )
            for method, res in results.items():
                rows.append({
                    'timestamp': timestamp,
                    'mode': 'ci-cost',
                    'target': target,
                    'dataset': data_file,
                    'alpha': alpha,
                    'experiments': n_experiments,
                    'ci_width': args.ci_width,
                    'ci_tolerance': args.ci_tolerance,
                    'method': method,
                    'achieved': bool(res.get('achieved', False)),
                    'required_labels': res.get('required_labels'),
                    'avg_ci_length': res.get('avg_ci_length'),
                    'gamma': res.get('gamma'),
                    'mse_snapshot': (res.get('results_snapshot') or {}).get('mse'),
                    'coverage_snapshot': (res.get('results_snapshot') or {}).get('coverage_rate'),
                    'variance_snapshot': (res.get('results_snapshot') or {}).get('variance'),
                    'adaptive_batch_size': res.get('batch_size'),
                    'adaptive_update_freq': res.get('design_update_freq'),
                    'adaptive_warmup_batches': res.get('warmup_batches'),
                    'settings_key': settings_key,
                })
            df_new = pd.DataFrame(rows)
            try:
                df_old = pd.read_csv(args.results_csv)
                keep_mask = ~df_old.apply(lambda r: (str(r.get('settings_key')) == settings_key) and (r.get('method') in df_new['method'].values), axis=1)
                df_merged = pd.concat([df_old[keep_mask], df_new], ignore_index=True)
            except FileNotFoundError:
                df_merged = df_new
            df_merged.to_csv(args.results_csv, index=False)
            logger.info(f"CIæˆæœ¬å¯¹æ¯”ç»“æœå·²å†™å…¥CSV: {args.results_csv}")
        except Exception as e:
            logger.warning(f"å†™å…¥CSVå¤±è´¥: {e}")
        return

    # å¦åˆ™è¿è¡Œnotebooké£æ ¼å¯¹æ¯”
    results = run_notebook_style_comparison(
        df,
        target=target,
        n_experiments=n_experiments,
        n_true_labels=n_true_labels,
        gamma=gamma,
        alpha=alpha,
        seed=seed
    )
    
    # ç”Ÿæˆå¯è§†åŒ–
    create_comparison_plots(results)
    
    # ä¿å­˜ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    if output:
        output_file = f'{output}_{target.lower()}_{timestamp}.json'
    else:
        output_file = f'estimator_comparison_{target.lower()}_{timestamp}.json'
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®ï¼ˆç§»é™¤numpyæ•°ç»„ï¼‰
    save_data = {}
    for method in ['PGAE', 'Adaptive_PGAE', 'Active_Inference']:
        if method in results:
            save_data[method] = {
                'mse': float(results[method]['mse']),
                'bias': float(results[method]['bias']),
                'variance': float(results[method]['variance']),
                'avg_ci_length': float(results[method]['avg_ci_length']),
                'coverage_rate': float(results[method]['coverage_rate']),
                'execution_time': float(results[method]['execution_time']),
                'n_experiments': int(results[method]['n_experiments'])
            }
    
    save_data['summary'] = results['summary']
    
    with open(output_file, 'w') as f:
        json.dump(save_data, f, indent=2)
    
    print(f"\nâœ… {target} ç»Ÿè®¡ä¼°è®¡å™¨å¯¹æ¯”å®Œæˆ!")
    print(f"é¢„æµ‹ç›®æ ‡: {target}")
    print(f"å®éªŒè®¾ç½®: {n_experiments} experiments, {n_true_labels} labels per experiment")
    print(f"ç»“æœæ–‡ä»¶: {output_file}")
    print(f"å¯è§†åŒ–æ–‡ä»¶: estimator_comparison.png")

    # ä¿å­˜åˆ°CSVï¼ˆæŒ‰è®¾ç½®å»é‡è¦†ç›–ï¼‰
    try:
        import pandas as pd
        rows = []
        settings_key = (
            f"mode=mse-compare|target={target}|dataset={data_file}|alpha={alpha}|experiments={n_experiments}|"
            f"labels={n_true_labels}|gamma={gamma}"
        )
        for method in ['PGAE', 'Adaptive_PGAE', 'Active_Inference', 'Naive']:
            if method in results and isinstance(results[method], dict) and 'mse' in results[method]:
                r = results[method]
                rows.append({
                    'timestamp': timestamp,
                    'mode': 'mse-compare',
                    'target': target,
                    'dataset': data_file,
                    'alpha': alpha,
                    'experiments': n_experiments,
                    'n_true_labels': n_true_labels,
                    'method': method,
                    'gamma': gamma,
                    'mse': r.get('mse'),
                    'avg_ci_length': r.get('avg_ci_length'),
                    'coverage_rate': r.get('coverage_rate'),
                    'execution_time': r.get('execution_time'),
                    'settings_key': settings_key,
                })
        if rows:
            df_new = pd.DataFrame(rows)
            try:
                df_old = pd.read_csv(args.results_csv)
                keep_mask = ~df_old.apply(lambda r: (str(r.get('settings_key')) == settings_key) and (r.get('method') in df_new['method'].values), axis=1)
                df_merged = pd.concat([df_old[keep_mask], df_new], ignore_index=True)
            except FileNotFoundError:
                df_merged = df_new
            df_merged.to_csv(args.results_csv, index=False)
            logger.info(f"MSEæ¯”è¾ƒç»“æœå·²å†™å…¥CSV: {args.results_csv}")
    except Exception as e:
        logger.warning(f"å†™å…¥CSVå¤±è´¥: {e}")
    
    # æ˜¾ç¤ºæœ€ä½³æ€§èƒ½æ–¹æ³•
    method_results = {k: v for k, v in results.items() if k not in ['summary'] and isinstance(v, dict) and 'mse' in v}
    if method_results:
        # é€æ–¹æ³•æ‰“å°MSE/è¦†ç›–ç‡/CI
        print("\nå„æ–¹æ³•æŒ‡æ ‡æ‘˜è¦:")
        for m, r in method_results.items():
            try:
                print(f"  {m:<18} MSE={r['mse']:.6f} | Coverage={r['coverage_rate']:.4f} | CI={r['avg_ci_length']:.4f}")
            except Exception:
                pass
        best_method = min(method_results.keys(), key=lambda x: method_results[x]['mse'])
        best_mse = method_results[best_method]['mse']
        best_coverage = method_results[best_method]['coverage_rate']
        print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_method}")
        print(f"   MSE: {best_mse:.6f}")
        print(f"   è¦†ç›–ç‡: {best_coverage:.4f}")

if __name__ == "__main__":
    main()
