#!/usr/bin/env python3
"""
æ•°å­—å­ªç”Ÿæ•°æ®é›†ä¸‹è½½ä¸å¤„ç†è„šæœ¬
Digital Twin Dataset Download and Processing Script

ä»Hugging Faceä¸‹è½½å’Œå¤„ç†LLM-Digital-Twin/Twin-2K-500æ•°æ®é›†
"""

import os
import json
import sys
import shutil
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import pandas as pd

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def check_and_install_datasets():
    """æ£€æŸ¥å¹¶å®‰è£…datasetsåº“"""
    try:
        from datasets import load_dataset
        logger.info("âœ… datasetsåº“å·²å®‰è£…")
        return load_dataset
    except ImportError:
        logger.info("æ­£åœ¨å®‰è£…datasetsåº“...")
        import subprocess
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "datasets"])
            from datasets import load_dataset
            logger.info("âœ… datasetsåº“å®‰è£…æˆåŠŸ")
            return load_dataset
        except Exception as e:
            logger.error(f"âŒ datasetsåº“å®‰è£…å¤±è´¥: {e}")
            raise

def clear_dataset_cache():
    """æ¸…ç†æ•°æ®é›†ç¼“å­˜"""
    try:
        cache_dir = Path.home() / ".cache" / "huggingface" / "datasets" / "LLM-Digital-Twin___parquet"
        if cache_dir.exists():
            logger.info(f"æ¸…ç†ç¼“å­˜ç›®å½•: {cache_dir}")
            shutil.rmtree(cache_dir)
            return True
    except Exception as e:
        logger.warning(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
        return False

def load_personas(num_personas: int = 30, force_reload: bool = False) -> Dict[str, str]:
    """
    ä»Hugging FaceåŠ è½½personaæ•°æ®
    
    Args:
        num_personas: åŠ è½½çš„personaæ•°é‡
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
    
    Returns:
        Dict[str, str]: persona_id -> persona_summaryçš„æ˜ å°„
    """
    load_dataset = check_and_install_datasets()
    
    logger.info(f"æ­£åœ¨åŠ è½½ {num_personas} ä¸ªpersonaæ‘˜è¦...")
    
    try:
        # å°è¯•åŠ è½½æ•°æ®é›†
        dataset = load_dataset("LLM-Digital-Twin/Twin-2K-500", 'full_persona', split='data')
        logger.info("âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        logger.warning(f"âš ï¸ åŠ è½½æ•°æ®é›†å¤±è´¥: {type(e).__name__}: {str(e)}")
        
        if force_reload or input("æ˜¯å¦æ¸…ç†ç¼“å­˜å¹¶é‡æ–°ä¸‹è½½ï¼Ÿ(y/n): ").lower() == 'y':
            clear_dataset_cache()
            
            try:
                dataset = load_dataset(
                    "LLM-Digital-Twin/Twin-2K-500", 
                    'full_persona', 
                    split='data', 
                    download_mode='force_redownload'
                )
                logger.info("âœ… é‡æ–°ä¸‹è½½æˆåŠŸ")
            except Exception as e2:
                logger.error(f"âŒ é‡æ–°ä¸‹è½½ä»ç„¶å¤±è´¥: {type(e2).__name__}: {str(e2)}")
                logger.error("è¯·æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†: https://huggingface.co/datasets/LLM-Digital-Twin/Twin-2K-500")
                raise e2
        else:
            raise e
    
    # æå–personas
    personas = {}
    pids = dataset["pid"]
    persona_summaries = dataset["persona_summary"]
    
    # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨æ•°æ®é‡
    max_available = len(pids)
    actual_num = min(num_personas, max_available)
    
    logger.info(f"æ•°æ®é›†åŒ…å« {max_available} ä¸ªpersonaï¼Œå°†åŠ è½½ {actual_num} ä¸ª")
    
    # åŠ è½½æŒ‡å®šæ•°é‡çš„personas
    for i in range(actual_num):
        pid = pids[i]
        summary = persona_summaries[i]
        
        if summary is not None:
            personas[f"pid_{pid}"] = summary
    
    logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(personas)} ä¸ªpersonas")
    return personas

def load_ground_truth_data(force_reload: bool = False) -> List:
    """
    åŠ è½½ground truthæ•°æ®
    
    Args:
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
    
    Returns:
        List: ground truthå“åº”æ•°æ®
    """
    load_dataset = check_and_install_datasets()
    
    logger.info("æ­£åœ¨åŠ è½½ground truthæ•°æ®...")
    
    try:
        wave_split = load_dataset("LLM-Digital-Twin/Twin-2K-500", "wave_split")
        ground_truth = wave_split["data"]["wave4_Q_wave4_A"]
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(ground_truth)} æ¡ground truthè®°å½•")
        return ground_truth
        
    except Exception as e:
        logger.error(f"âŒ åŠ è½½ground truthæ•°æ®å¤±è´¥: {e}")
        if force_reload:
            clear_dataset_cache()
            try:
                wave_split = load_dataset(
                    "LLM-Digital-Twin/Twin-2K-500", 
                    "wave_split",
                    download_mode='force_redownload'
                )
                ground_truth = wave_split["data"]["wave4_Q_wave4_A"]
                logger.info(f"âœ… é‡æ–°ä¸‹è½½æˆåŠŸï¼ŒåŠ è½½ {len(ground_truth)} æ¡è®°å½•")
                return ground_truth
            except Exception as e2:
                logger.error(f"âŒ é‡æ–°ä¸‹è½½å¤±è´¥: {e2}")
                raise e2
        else:
            raise e

def save_ground_truth_to_json(ground_truth: List, filename: str = "ground_truth_output.json") -> None:
    """
    ä¿å­˜ground truthæ•°æ®åˆ°JSONæ–‡ä»¶
    
    Args:
        ground_truth: ground truthæ•°æ®
        filename: è¾“å‡ºæ–‡ä»¶å
    """
    logger.info(f"æ­£åœ¨ä¿å­˜ground truthæ•°æ®åˆ° {filename}...")
    
    parsed = []
    for i, entry in enumerate(ground_truth):
        try:
            if isinstance(entry, str):
                parsed.append(json.loads(entry))
            else:
                parsed.append(entry)
        except Exception as e:
            logger.warning(f"è§£æç¬¬ {i} æ¡è®°å½•å¤±è´¥: {e}")
            parsed.append({"error": str(e), "raw": entry})

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path = Path(filename)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(parsed, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ… å·²ä¿å­˜åˆ° {filename}")

def save_personas_to_json(personas: Dict[str, str], filename: str = "personas_output.json") -> None:
    """
    ä¿å­˜personasæ•°æ®åˆ°JSONæ–‡ä»¶
    
    Args:
        personas: personaså­—å…¸
        filename: è¾“å‡ºæ–‡ä»¶å
    """
    logger.info(f"æ­£åœ¨ä¿å­˜ {len(personas)} ä¸ªpersonasåˆ° {filename}...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path = Path(filename)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(personas, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ… å·²ä¿å­˜åˆ° {filename}")

def analyze_persona_sample(personas: Dict[str, str], show_first_n: int = 3) -> None:
    """
    åˆ†æå¹¶æ˜¾ç¤ºpersonaæ ·æœ¬
    
    Args:
        personas: personaså­—å…¸
        show_first_n: æ˜¾ç¤ºå‰Nä¸ªpersonaçš„æ•°é‡
    """
    if not personas:
        logger.warning("æ²¡æœ‰å¯åˆ†æçš„personaæ•°æ®")
        return
    
    logger.info("=" * 50)
    logger.info("PERSONA æ ·æœ¬åˆ†æ")
    logger.info("=" * 50)
    
    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    logger.info(f"æ€»personaæ•°é‡: {len(personas)}")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªpersonaçš„æ‘˜è¦
    for i, (pid, summary) in enumerate(list(personas.items())[:show_first_n]):
        logger.info(f"\nã€{pid}ã€‘")
        logger.info(f"æ‘˜è¦é•¿åº¦: {len(summary)} å­—ç¬¦")
        logger.info(f"å‰500å­—ç¬¦: {summary[:500]}...")
        
        # ç®€å•åˆ†æå…³é”®ä¿¡æ¯
        demographics = extract_demographics_info(summary)
        if demographics:
            logger.info(f"å…³é”®ä¿¡æ¯: {demographics}")

def extract_demographics_info(persona_text: str) -> Dict[str, str]:
    """
    ä»personaæ–‡æœ¬ä¸­æå–å…³é”®äººå£ç»Ÿè®¡å­¦ä¿¡æ¯
    
    Args:
        persona_text: personaæè¿°æ–‡æœ¬
        
    Returns:
        Dict[str, str]: æå–çš„å…³é”®ä¿¡æ¯
    """
    info = {}
    
    # ç®€å•çš„å…³é”®è¯åŒ¹é…æå–
    lines = persona_text.split('\n')
    for line in lines:
        line = line.strip()
        if ':' in line:
            key, value = line.split(':', 1)
            key = key.strip()
            value = value.strip()
            
            # æå–å…³é”®å­—æ®µ
            if any(keyword in key.lower() for keyword in ['gender', 'age', 'education', 'income', 'political']):
                info[key] = value
    
    return info

def validate_data_integrity(personas: Dict[str, str], ground_truth: List) -> bool:
    """
    éªŒè¯æ•°æ®å®Œæ•´æ€§
    
    Args:
        personas: personasæ•°æ®
        ground_truth: ground truthæ•°æ®
        
    Returns:
        bool: éªŒè¯æ˜¯å¦é€šè¿‡
    """
    logger.info("æ­£åœ¨éªŒè¯æ•°æ®å®Œæ•´æ€§...")
    
    issues = []
    
    # æ£€æŸ¥personas
    if not personas:
        issues.append("Personasæ•°æ®ä¸ºç©º")
    else:
        empty_personas = [pid for pid, summary in personas.items() if not summary or len(summary.strip()) < 50]
        if empty_personas:
            issues.append(f"å‘ç° {len(empty_personas)} ä¸ªç©ºæˆ–è¿‡çŸ­çš„persona: {empty_personas[:5]}")
    
    # æ£€æŸ¥ground truth
    if not ground_truth:
        issues.append("Ground truthæ•°æ®ä¸ºç©º")
    else:
        try:
            # å°è¯•è§£æç¬¬ä¸€æ¡è®°å½•
            first_record = ground_truth[0]
            if isinstance(first_record, str):
                json.loads(first_record)
            logger.info(f"Ground truthåŒ…å« {len(ground_truth)} æ¡è®°å½•")
        except Exception as e:
            issues.append(f"Ground truthæ•°æ®æ ¼å¼å¼‚å¸¸: {e}")
    
    if issues:
        logger.warning("æ•°æ®éªŒè¯å‘ç°é—®é¢˜:")
        for issue in issues:
            logger.warning(f"  - {issue}")
        return False
    else:
        logger.info("âœ… æ•°æ®éªŒè¯é€šè¿‡")
        return True

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°å­—å­ªç”Ÿæ•°æ®é›†å¤„ç†å·¥å…·')
    parser.add_argument('--num-personas', type=int, default=30, help='åŠ è½½çš„personaæ•°é‡')
    parser.add_argument('--output-dir', default='./data', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--force-reload', action='store_true', help='å¼ºåˆ¶é‡æ–°ä¸‹è½½')
    parser.add_argument('--skip-ground-truth', action='store_true', help='è·³è¿‡ground truthä¸‹è½½')
    parser.add_argument('--analyze-only', action='store_true', help='ä»…åˆ†æç°æœ‰æ•°æ®ï¼Œä¸ä¸‹è½½')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        if not args.analyze_only:
            # 1. åŠ è½½personas
            logger.info("å¼€å§‹å¤„ç†æ•°å­—å­ªç”Ÿæ•°æ®é›†...")
            personas = load_personas(args.num_personas, args.force_reload)
            
            # ä¿å­˜personas
            personas_file = output_dir / "personas_output.json"
            save_personas_to_json(personas, str(personas_file))
            
            # 2. åŠ è½½ground truth (å¯é€‰)
            if not args.skip_ground_truth:
                ground_truth = load_ground_truth_data(args.force_reload)
                
                # ä¿å­˜ground truth
                gt_file = output_dir / "ground_truth_output.json"
                save_ground_truth_to_json(ground_truth, str(gt_file))
            else:
                logger.info("è·³è¿‡ground truthæ•°æ®ä¸‹è½½")
                ground_truth = []
            
            # 3. éªŒè¯æ•°æ®
            validate_data_integrity(personas, ground_truth)
            
            # 4. åˆ†ææ ·æœ¬
            analyze_persona_sample(personas)
        
        else:
            # ä»…åˆ†ææ¨¡å¼
            logger.info("åˆ†ææ¨¡å¼ï¼šæ£€æŸ¥ç°æœ‰æ•°æ®æ–‡ä»¶...")
            
            personas_file = output_dir / "personas_output.json"
            if personas_file.exists():
                with open(personas_file, 'r', encoding='utf-8') as f:
                    personas = json.load(f)
                analyze_persona_sample(personas)
            else:
                logger.warning(f"æœªæ‰¾åˆ°personasæ–‡ä»¶: {personas_file}")
        
        logger.info("ğŸ‰ æ•°æ®å¤„ç†å®Œæˆ!")
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()