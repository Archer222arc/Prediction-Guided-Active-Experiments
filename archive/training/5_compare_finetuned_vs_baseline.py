#!/usr/bin/env python3
"""
å¯¹æ¯”å¾®è°ƒæ¨¡å‹ä¸åŸºçº¿æ¨¡å‹çš„é¢„æµ‹æ•ˆæœ
ä½¿ç”¨åŸºç¡€æç¤ºæ–¹æ³•æµ‹è¯•åŒæ ·çš„ NPORS æ•°æ®
"""

import json
import pandas as pd
import numpy as np
from openai import AzureOpenAI
import time
from typing import Dict, List
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NPORSComparison:
    """NPORS é¢„æµ‹å¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self):
        # è¯»å–é…ç½®
        with open("config/azure_models_config.json", 'r') as f:
            config = json.load(f)
        
        # å¾®è°ƒæ¨¡å‹å®¢æˆ·ç«¯
        finetuned_config = config["azure_endpoints"]["deployed_finetuned"]
        self.finetuned_client = AzureOpenAI(
            api_version=finetuned_config["api_version"],
            azure_endpoint=finetuned_config["endpoint"],
            api_key=finetuned_config["api_key"]
        )
        self.finetuned_deployment = finetuned_config["deployment_name"]
        
        print(f"âœ… å¾®è°ƒæ¨¡å‹å·²è¿æ¥: {self.finetuned_deployment}")
        
        # é—®é¢˜æ˜ å°„ (ä¸åŸå§‹LLM_predictionä¿æŒä¸€è‡´)
        self.questions = {
            'ECON1MOD': "How would you rate the economic conditions in your community today? 1. Excellent, 2. Good, 3. Only fair, 4. Poor.",
            'UNITY': "Which statement comes closer to your own view, even if neither is exactly right? 1. Americans are united when it comes to the most important values. 2. Americans are divided when it comes to the most important values.",
            'GPT1': "Have you heard of ChatGPT? 1. Yes, 2. No, 3. Not sure.",
            'MOREGUNIMPACT': "If more Americans owned guns, do you think there would be... 1. More crime, 2. Less crime, 3. No difference.",
            'GAMBLERESTR': "How much government regulation of gambling do you favor? 1. A lot more than now, 2. A little more than now, 3. About the same as now, 4. A little less than now, 5. A lot less than now."
        }
        
        # å“åº”èŒƒå›´æ˜ å°„
        self.response_ranges = {
            'ECON1MOD': [1, 2, 3, 4],
            'UNITY': [1, 2],
            'GPT1': [1, 2, 3],
            'MOREGUNIMPACT': [1, 2, 3],
            'GAMBLERESTR': [1, 2, 3, 4, 5]
        }
    
    def build_demographic_prompt(self, row):
        """æ„å»ºäººå£ç»Ÿè®¡èƒŒæ™¯æç¤º (ä¸åŸå§‹LLM_predictionä¿æŒä¸€è‡´)"""
        
        # å¤„ç†æ€§åˆ«
        gender_map = {1: "Man", 2: "Woman", 3: "Some other way"}
        gender = gender_map.get(row['GENDER'], "Unknown")
        
        # å¤„ç†å‡ºç”Ÿåœ°
        birthplace_map = {
            1: "50 U.S. states or D.C.",
            2: "Puerto Rico", 
            3: "A U.S. territory",
            4: "Another country other than U.S."
        }
        birthplace = birthplace_map.get(row['BIRTHPLACE'], "Unknown")
        
        # å¤„ç†å©šå§»çŠ¶å†µ
        marital_map = {
            1: "Married", 2: "Living with a partner", 3: "Divorced",
            4: "Separated", 5: "Widowed", 6: "Never married"
        }
        marital = marital_map.get(row['MARITAL'], "Unknown")
        
        # å¤„ç†æ•™è‚²æ°´å¹³
        education_map = {
            1: "No formal education", 2: "1st-8th grade", 3: "Some high school",
            4: "High school graduate", 5: "Some college", 6: "Bachelor's degree",
            7: "Postgraduate degree"
        }
        education = education_map.get(row['EDUCATION'], "Unknown")
        
        # å¤„ç†æ”¶å…¥
        income_map = {
            1: "Less than $30,000", 2: "$30,000â€“39,999", 3: "$40,000â€“49,999",
            4: "$50,000â€“59,999", 5: "$60,000â€“69,999", 6: "$70,000â€“79,999",
            7: "$80,000â€“89,999", 8: "$90,000â€“99,999", 9: "$100,000+"
        }
        income = income_map.get(row['INC_SDT1'], "Unknown")
        
        # å¤„ç†åœ°åŒº
        division_map = {
            1: "New England (CT, ME, MA, NH, RI, VT)",
            2: "Middle Atlantic (NJ, NY, PA)", 
            3: "East North Central (IL, IN, MI, OH, WI)",
            4: "West North Central (IA, KS, MN, MO, NE, ND, SD)",
            5: "South Atlantic (DE, DC, FL, GA, MD, NC, SC, VA, WV)",
            6: "East South Central (AL, KY, MS, TN)",
            7: "West South Central (AR, LA, OK, TX)",
            8: "Mountain (AZ, CO, ID, MT, NV, NM, UT, WY)",
            9: "Pacific (AK, CA, HI, OR, WA)"
        }
        region = division_map.get(row['DIVISION'], "Unknown")
        
        # å¤„ç†éƒ½å¸‚åŒºåŸŸ
        metro_map = {1: "Non-metropolitan area", 2: "Metropolitan area"}
        area_type = metro_map.get(row['METRO'], "Unknown")
        
        # æ„å»ºç³»ç»Ÿæç¤º (ä¸åŸå§‹æ–¹æ³•å®Œå…¨ä¸€è‡´)
        system_prompt = f"""You are a respondent in a survey at the time of May 1st, 2024. You are a {row['AGE']}-year-old {gender} who is {row['RACE_TEXT']}. You were born in {birthplace}, and are currently {marital}. You have an education level of {education}. Your annual household income is {income}. You live in the {region} region and are located in a {area_type} area. Answer survey questions based on your demographic profile and personal circumstances. Be realistic and consistent with your background."""
        
        return system_prompt
    
    def predict_with_finetuned(self, row, question_id: str) -> str:
        """ä½¿ç”¨å¾®è°ƒæ¨¡å‹é¢„æµ‹"""
        
        system_prompt = self.build_demographic_prompt(row)
        question_text = self.questions[question_id]
        user_prompt = f"Question: {question_text}\nPlease output the number only."
        
        try:
            response = self.finetuned_client.chat.completions.create(
                model=self.finetuned_deployment,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_completion_tokens=5,
                temperature=0.0
            )
            
            prediction = response.choices[0].message.content.strip()
            
            # éªŒè¯é¢„æµ‹ç»“æœ
            try:
                pred_num = int(prediction)
                if pred_num in self.response_ranges[question_id]:
                    return pred_num
                else:
                    logger.warning(f"Invalid prediction {pred_num} for {question_id}")
                    return None
            except ValueError:
                logger.warning(f"Non-numeric prediction: {prediction}")
                return None
                
        except Exception as e:
            logger.error(f"Prediction failed: {e}")
            return None
    
    def run_comparison_sample(self, sample_size: int = 100):
        """åœ¨æ ·æœ¬æ•°æ®ä¸Šè¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        
        # åŠ è½½åŸå§‹æ•°æ®
        logger.info("Loading NPORS data...")
        df = pd.read_csv('data/NPORS_2024_for_public_release_updated.csv')
        
        # åŠ è½½åŸºçº¿é¢„æµ‹ç»“æœ
        logger.info("Loading baseline predictions...")
        baseline_df = pd.read_csv('data/NPORS_2024_for_public_release_basic_prompting.csv')
        
        # ç¡®ä¿æœ‰åŸºçº¿ç»“æœçš„æ•°æ®
        baseline_respids = set(baseline_df['RESPID'].values)
        df_with_baseline = df[df['RESPID'].isin(baseline_respids)].copy()
        
        # éšæœºé‡‡æ ·
        if len(df_with_baseline) > sample_size:
            df_sample = df_with_baseline.sample(n=sample_size, random_state=42)
        else:
            df_sample = df_with_baseline.copy()
        
        logger.info(f"Testing on {len(df_sample)} samples")
        
        # ä¸ºæ¯ä¸ªé—®é¢˜è¿è¡Œé¢„æµ‹
        for question_id in self.questions.keys():
            logger.info(f"Predicting {question_id}...")
            
            finetuned_predictions = []
            
            for idx, row in df_sample.iterrows():
                if pd.isna(row[question_id]) or row[question_id] == 99.0:
                    finetuned_predictions.append(None)
                    continue
                
                # ä½¿ç”¨å¾®è°ƒæ¨¡å‹é¢„æµ‹
                pred = self.predict_with_finetuned(row, question_id)
                finetuned_predictions.append(pred)
                
                # é¿å…APIé™åˆ¶
                time.sleep(0.2)
            
            # æ·»åŠ åˆ°ç»“æœä¸­
            df_sample[f'{question_id}_FINETUNED'] = finetuned_predictions
        
        return df_sample
    
    def analyze_comparison(self, df_results):
        """åˆ†æå¯¹æ¯”ç»“æœ"""
        
        results = {}
        
        for question_id in self.questions.keys():
            baseline_col = f'{question_id}_LLM'
            finetuned_col = f'{question_id}_FINETUNED'
            actual_col = question_id
            
            # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
            valid_mask = (
                df_results[actual_col].notna() & 
                (df_results[actual_col] != 99.0) &
                df_results[baseline_col].notna() &
                df_results[finetuned_col].notna()
            )
            
            if valid_mask.sum() == 0:
                continue
            
            actual = df_results.loc[valid_mask, actual_col].astype(int)
            baseline = df_results.loc[valid_mask, baseline_col].astype(int) 
            finetuned = df_results.loc[valid_mask, finetuned_col].astype(int)
            
            # è®¡ç®—å‡†ç¡®ç‡
            baseline_accuracy = (actual == baseline).mean()
            finetuned_accuracy = (actual == finetuned).mean()
            
            # è®¡ç®—åˆ†å¸ƒç›¸ä¼¼æ€§ (KLæ•£åº¦)
            def calculate_distribution_similarity(actual, predicted):
                actual_dist = pd.Series(actual).value_counts(normalize=True).sort_index()
                pred_dist = pd.Series(predicted).value_counts(normalize=True).reindex(actual_dist.index, fill_value=0.001)
                
                # KLæ•£åº¦ (è¶Šå°è¶Šå¥½)
                kl_div = np.sum(actual_dist * np.log(actual_dist / pred_dist))
                return kl_div
            
            baseline_kl = calculate_distribution_similarity(actual, baseline)
            finetuned_kl = calculate_distribution_similarity(actual, finetuned)
            
            results[question_id] = {
                'sample_size': len(actual),
                'baseline_accuracy': baseline_accuracy,
                'finetuned_accuracy': finetuned_accuracy,
                'accuracy_improvement': finetuned_accuracy - baseline_accuracy,
                'baseline_kl_divergence': baseline_kl,
                'finetuned_kl_divergence': finetuned_kl,
                'kl_improvement': baseline_kl - finetuned_kl
            }
        
        return results
    
    def print_comparison_report(self, results):
        """æ‰“å°å¯¹æ¯”æŠ¥å‘Š"""
        
        print("\n" + "="*80)
        print("å¾®è°ƒæ¨¡å‹ vs åŸºçº¿æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š")
        print("="*80)
        
        for question_id, metrics in results.items():
            print(f"\nğŸ“Š {question_id} - {self.questions[question_id][:50]}...")
            print(f"   æ ·æœ¬æ•°é‡: {metrics['sample_size']}")
            print(f"   åŸºçº¿å‡†ç¡®ç‡: {metrics['baseline_accuracy']:.3f}")
            print(f"   å¾®è°ƒå‡†ç¡®ç‡: {metrics['finetuned_accuracy']:.3f}")
            print(f"   å‡†ç¡®ç‡æå‡: {metrics['accuracy_improvement']:+.3f} ({metrics['accuracy_improvement']*100:+.1f}%)")
            print(f"   åŸºçº¿KLæ•£åº¦: {metrics['baseline_kl_divergence']:.3f}")
            print(f"   å¾®è°ƒKLæ•£åº¦: {metrics['finetuned_kl_divergence']:.3f}")
            print(f"   åˆ†å¸ƒç›¸ä¼¼æ€§æå‡: {metrics['kl_improvement']:+.3f}")
            
            if metrics['accuracy_improvement'] > 0:
                print("   âœ… å¾®è°ƒæ¨¡å‹æ›´å‡†ç¡®")
            elif metrics['accuracy_improvement'] < 0:
                print("   âš ï¸ åŸºçº¿æ¨¡å‹æ›´å‡†ç¡®")
            else:
                print("   â– å‡†ç¡®ç‡ç›¸åŒ")
        
        # æ€»ä½“ç»Ÿè®¡
        total_baseline_acc = np.mean([m['baseline_accuracy'] for m in results.values()])
        total_finetuned_acc = np.mean([m['finetuned_accuracy'] for m in results.values()])
        total_improvement = total_finetuned_acc - total_baseline_acc
        
        print(f"\nğŸ¯ æ€»ä½“è¡¨ç°:")
        print(f"   å¹³å‡åŸºçº¿å‡†ç¡®ç‡: {total_baseline_acc:.3f}")
        print(f"   å¹³å‡å¾®è°ƒå‡†ç¡®ç‡: {total_finetuned_acc:.3f}")
        print(f"   å¹³å‡å‡†ç¡®ç‡æå‡: {total_improvement:+.3f} ({total_improvement*100:+.1f}%)")
        
        if total_improvement > 0.01:
            print("   ğŸ‰ å¾®è°ƒæ˜¾è‘—æå‡äº†é¢„æµ‹æ•ˆæœ!")
        elif total_improvement > 0:
            print("   âœ… å¾®è°ƒç•¥å¾®æå‡äº†é¢„æµ‹æ•ˆæœ")
        else:
            print("   âŒ å¾®è°ƒæ²¡æœ‰æå‡é¢„æµ‹æ•ˆæœ")

def main():
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•:")
        print("  python 5_compare_finetuned_vs_baseline.py run [sample_size]    # è¿è¡Œå¯¹æ¯”æµ‹è¯•")
        print("  python 5_compare_finetuned_vs_baseline.py analyze              # åˆ†æå·²æœ‰ç»“æœ")
        return
    
    action = sys.argv[1]
    
    comparator = NPORSComparison()
    
    if action == "run":
        sample_size = int(sys.argv[2]) if len(sys.argv) > 2 else 50
        
        print(f"è¿è¡Œå¯¹æ¯”æµ‹è¯• (æ ·æœ¬æ•°: {sample_size})")
        
        # è¿è¡Œå¯¹æ¯”
        df_results = comparator.run_comparison_sample(sample_size)
        
        # ä¿å­˜ç»“æœ
        output_file = f"comparison_results_sample_{sample_size}.csv"
        df_results.to_csv(output_file, index=False)
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # åˆ†æç»“æœ
        results = comparator.analyze_comparison(df_results)
        
        # æ‰“å°æŠ¥å‘Š
        comparator.print_comparison_report(results)
        
        # ä¿å­˜åˆ†æç»“æœ
        with open(f"comparison_analysis_sample_{sample_size}.json", "w") as f:
            json.dump(results, f, indent=2)
    
    elif action == "analyze":
        # åˆ†æç°æœ‰ç»“æœæ–‡ä»¶
        try:
            df_results = pd.read_csv("comparison_results_sample_50.csv")
            results = comparator.analyze_comparison(df_results)
            comparator.print_comparison_report(results)
        except FileNotFoundError:
            print("æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ 'run' å‘½ä»¤")

if __name__ == "__main__":
    main()